[
  {
    "objectID": "bayes_intro.html",
    "href": "bayes_intro.html",
    "title": "Intro to Bayesian Inference in R",
    "section": "",
    "text": "Suppose we flip a coin a set number of times and count up the number of times a head appears. In this first example, we don’t have any strong belief about the probability that a head will appear. In other words, any probability between 0 and 1 is equally likely to occur. Based on this prior knowledge (or lack thereof), we can visualize the prior distribution of potential probabilities drawing samples from a uniform distribution between 0 and 1.\n\nhist(runif(10000,0,1), breaks = 30, freq = FALSE, main = \"Prior Distribution\",\n       xlab = \"Probability of Heads\", ylab = \"Density\", xlim = c(0, 1))\n\n\n\n\nUsing this prior knowledge and the binomial likelihood function for our data we can generate samples from the posterior distribution.\n\ncoin_model_unif &lt;- function(n_flips){\n  # Simulate coin flips (0 for tails, 1 for heads)\n  set.seed(5)\n  coin_flips &lt;- sample(c(0, 1), n_flips, replace = TRUE)\n  \n  # Likelihood function: Binomial likelihood\n  likelihood &lt;- function(p) {\n    sum(dbinom(coin_flips, size = 1, prob = p, log = TRUE))\n  }\n  \n  # Prior distribution: Uniform prior\n  prior &lt;- function(p) {\n    dunif(p, 0, 1, log = TRUE)\n  }\n  \n  # Posterior function: Proportional to the product of prior and likelihood\n  posterior &lt;- function(p) {\n    prior(p) + likelihood(p)\n  }\n  \n  # Metropolis-Hastings MCMC algorithm\n  metropolis_hastings &lt;- function() {\n    samples &lt;- numeric(10000)\n    current_sample &lt;- runif(1)  # Initialize at random value between 0 and 1\n    \n    for (i in 1:10000) {\n      # Sample proposed sample from a truncated normal distribution\n      proposed_sample &lt;- truncnorm::rtruncnorm(1, a = 0, b = 1, \n                                               mean = current_sample, sd = 0.1)\n      \n      # Calculate acceptance probability\n      acceptance_prob &lt;- exp(posterior(proposed_sample) - \n                               posterior(current_sample))\n      \n      # Accept or reject proposal\n      if (runif(1) &lt; acceptance_prob) {\n        current_sample &lt;- proposed_sample\n      }\n      \n      samples[i] &lt;- current_sample\n    }\n    \n    return(samples)\n  }\n  \n  # Run Metropolis-Hastings MCMC\n  posterior_samples &lt;- metropolis_hastings()\n  \n  # Plot posterior distribution\n  hist(posterior_samples, breaks = 30, freq = FALSE, \n       main = \"Posterior Distribution\",\n       xlab = \"Probability of Heads\", ylab = \"Density\", xlim = c(0, 1))\n  observed_frequency &lt;- sum(coin_flips) / n_flips\n  abline(v = observed_frequency, col = \"red\", lwd = 2)\n  legend(\"topright\", legend = \"Observed Frequency of Heads\", col = \"red\", lty = 1,\n         lwd = 2)\n\n}\n\ncoin_model_unif(10)\n\n\n\ncoin_model_unif(100)\n\n\n\n\nWhen we don’t have any prior information, Bayesian inference resembles classical inference. The above estimates can be obtained from maximum likelihood.\nIf we go into this problem with some prior knowledge about the probability of flipping heads, this knowledge will influence our posterior distribution. If we are pretty confident that the probability is 50%, but we want to leave some room for the possibility that the coin is biased, we might specify a prior distribution that looks like this (beta distribution centered at 0.5).\n\nhist(rbeta(10000,20,20), breaks = 30, freq = FALSE, main = \"Prior Distribution\",\n       xlab = \"Probability of Heads\", ylab = \"Density\", xlim = c(0, 1))\n\n\n\n\nNow we can repeat the steps from earlier to generate samples from the posterior distribution.\n\ncoin_model_beta &lt;- function(n_flips){\n  # Simulate coin flips (0 for tails, 1 for heads)\n  set.seed(5)\n  coin_flips &lt;- sample(c(0, 1), n_flips, replace = TRUE)\n  \n  # Likelihood function: Binomial likelihood\n  likelihood &lt;- function(p) {\n    sum(dbinom(coin_flips, size = 1, prob = p, log = TRUE))\n  }\n  \n  # Prior distribution: Beta prior centered on 0.5\n  prior &lt;- function(p) {\n    dbeta(p, 20, 20, log = TRUE)\n  }\n  \n  # Posterior function: Proportional to the product of prior and likelihood\n  posterior &lt;- function(p) {\n    prior(p) + likelihood(p)\n  }\n  \n  # Metropolis-Hastings MCMC algorithm\n  metropolis_hastings &lt;- function() {\n    samples &lt;- numeric(10000)\n    current_sample &lt;- runif(1)  # Initialize at random value between 0 and 1\n    \n    for (i in 1:10000) {\n      # Sample proposed sample from a truncated normal distribution\n      proposed_sample &lt;- truncnorm::rtruncnorm(1, a = 0, b = 1, \n                                               mean = current_sample, sd = 0.1)\n      \n      # Calculate acceptance probability\n      acceptance_prob &lt;- exp(posterior(proposed_sample) - \n                               posterior(current_sample))\n      \n      # Accept or reject proposal\n      if (runif(1) &lt; acceptance_prob) {\n        current_sample &lt;- proposed_sample\n      }\n      \n      samples[i] &lt;- current_sample\n    }\n    \n    return(samples)\n  }\n  \n  # Run Metropolis-Hastings MCMC\n  posterior_samples &lt;- metropolis_hastings()\n  \n  # Plot posterior distribution\n  hist(posterior_samples, breaks = 30, freq = FALSE, \n       main = \"Posterior Distribution\",\n       xlab = \"Probability of Heads\", ylab = \"Density\", xlim = c(0, 1))\n  observed_frequency &lt;- sum(coin_flips) / n_flips\n  abline(v = observed_frequency, col = \"red\", lwd = 2)\n  legend(\"topright\", legend = \"Observed Frequency of Heads\", col = \"red\", lty = 1,\n         lwd = 2)\n\n}\n\ncoin_model_beta(10)\n\n\n\ncoin_model_beta(100)\n\n\n\n\nWe can see that the prior has a lot of influence on the posterior distribution when we only flip the coin 10 times, but is less influential when we have more data and the observed frequency is closer to 50%."
  },
  {
    "objectID": "bayes_intro.html#example-flipping-a-coin",
    "href": "bayes_intro.html#example-flipping-a-coin",
    "title": "Intro to Bayesian Inference in R",
    "section": "",
    "text": "Suppose we flip a coin a set number of times and count up the number of times a head appears. In this first example, we don’t have any strong belief about the probability that a head will appear. In other words, any probability between 0 and 1 is equally likely to occur. Based on this prior knowledge (or lack thereof), we can visualize the prior distribution of potential probabilities drawing samples from a uniform distribution between 0 and 1.\n\nhist(runif(10000,0,1), breaks = 30, freq = FALSE, main = \"Prior Distribution\",\n       xlab = \"Probability of Heads\", ylab = \"Density\", xlim = c(0, 1))\n\n\n\n\nUsing this prior knowledge and the binomial likelihood function for our data we can generate samples from the posterior distribution.\n\ncoin_model_unif &lt;- function(n_flips){\n  # Simulate coin flips (0 for tails, 1 for heads)\n  set.seed(5)\n  coin_flips &lt;- sample(c(0, 1), n_flips, replace = TRUE)\n  \n  # Likelihood function: Binomial likelihood\n  likelihood &lt;- function(p) {\n    sum(dbinom(coin_flips, size = 1, prob = p, log = TRUE))\n  }\n  \n  # Prior distribution: Uniform prior\n  prior &lt;- function(p) {\n    dunif(p, 0, 1, log = TRUE)\n  }\n  \n  # Posterior function: Proportional to the product of prior and likelihood\n  posterior &lt;- function(p) {\n    prior(p) + likelihood(p)\n  }\n  \n  # Metropolis-Hastings MCMC algorithm\n  metropolis_hastings &lt;- function() {\n    samples &lt;- numeric(10000)\n    current_sample &lt;- runif(1)  # Initialize at random value between 0 and 1\n    \n    for (i in 1:10000) {\n      # Sample proposed sample from a truncated normal distribution\n      proposed_sample &lt;- truncnorm::rtruncnorm(1, a = 0, b = 1, \n                                               mean = current_sample, sd = 0.1)\n      \n      # Calculate acceptance probability\n      acceptance_prob &lt;- exp(posterior(proposed_sample) - \n                               posterior(current_sample))\n      \n      # Accept or reject proposal\n      if (runif(1) &lt; acceptance_prob) {\n        current_sample &lt;- proposed_sample\n      }\n      \n      samples[i] &lt;- current_sample\n    }\n    \n    return(samples)\n  }\n  \n  # Run Metropolis-Hastings MCMC\n  posterior_samples &lt;- metropolis_hastings()\n  \n  # Plot posterior distribution\n  hist(posterior_samples, breaks = 30, freq = FALSE, \n       main = \"Posterior Distribution\",\n       xlab = \"Probability of Heads\", ylab = \"Density\", xlim = c(0, 1))\n  observed_frequency &lt;- sum(coin_flips) / n_flips\n  abline(v = observed_frequency, col = \"red\", lwd = 2)\n  legend(\"topright\", legend = \"Observed Frequency of Heads\", col = \"red\", lty = 1,\n         lwd = 2)\n\n}\n\ncoin_model_unif(10)\n\n\n\ncoin_model_unif(100)\n\n\n\n\nWhen we don’t have any prior information, Bayesian inference resembles classical inference. The above estimates can be obtained from maximum likelihood.\nIf we go into this problem with some prior knowledge about the probability of flipping heads, this knowledge will influence our posterior distribution. If we are pretty confident that the probability is 50%, but we want to leave some room for the possibility that the coin is biased, we might specify a prior distribution that looks like this (beta distribution centered at 0.5).\n\nhist(rbeta(10000,20,20), breaks = 30, freq = FALSE, main = \"Prior Distribution\",\n       xlab = \"Probability of Heads\", ylab = \"Density\", xlim = c(0, 1))\n\n\n\n\nNow we can repeat the steps from earlier to generate samples from the posterior distribution.\n\ncoin_model_beta &lt;- function(n_flips){\n  # Simulate coin flips (0 for tails, 1 for heads)\n  set.seed(5)\n  coin_flips &lt;- sample(c(0, 1), n_flips, replace = TRUE)\n  \n  # Likelihood function: Binomial likelihood\n  likelihood &lt;- function(p) {\n    sum(dbinom(coin_flips, size = 1, prob = p, log = TRUE))\n  }\n  \n  # Prior distribution: Beta prior centered on 0.5\n  prior &lt;- function(p) {\n    dbeta(p, 20, 20, log = TRUE)\n  }\n  \n  # Posterior function: Proportional to the product of prior and likelihood\n  posterior &lt;- function(p) {\n    prior(p) + likelihood(p)\n  }\n  \n  # Metropolis-Hastings MCMC algorithm\n  metropolis_hastings &lt;- function() {\n    samples &lt;- numeric(10000)\n    current_sample &lt;- runif(1)  # Initialize at random value between 0 and 1\n    \n    for (i in 1:10000) {\n      # Sample proposed sample from a truncated normal distribution\n      proposed_sample &lt;- truncnorm::rtruncnorm(1, a = 0, b = 1, \n                                               mean = current_sample, sd = 0.1)\n      \n      # Calculate acceptance probability\n      acceptance_prob &lt;- exp(posterior(proposed_sample) - \n                               posterior(current_sample))\n      \n      # Accept or reject proposal\n      if (runif(1) &lt; acceptance_prob) {\n        current_sample &lt;- proposed_sample\n      }\n      \n      samples[i] &lt;- current_sample\n    }\n    \n    return(samples)\n  }\n  \n  # Run Metropolis-Hastings MCMC\n  posterior_samples &lt;- metropolis_hastings()\n  \n  # Plot posterior distribution\n  hist(posterior_samples, breaks = 30, freq = FALSE, \n       main = \"Posterior Distribution\",\n       xlab = \"Probability of Heads\", ylab = \"Density\", xlim = c(0, 1))\n  observed_frequency &lt;- sum(coin_flips) / n_flips\n  abline(v = observed_frequency, col = \"red\", lwd = 2)\n  legend(\"topright\", legend = \"Observed Frequency of Heads\", col = \"red\", lty = 1,\n         lwd = 2)\n\n}\n\ncoin_model_beta(10)\n\n\n\ncoin_model_beta(100)\n\n\n\n\nWe can see that the prior has a lot of influence on the posterior distribution when we only flip the coin 10 times, but is less influential when we have more data and the observed frequency is closer to 50%."
  },
  {
    "objectID": "bayes_intro.html#modelling-salamander-larvae-abundance",
    "href": "bayes_intro.html#modelling-salamander-larvae-abundance",
    "title": "Intro to Bayesian Inference in R",
    "section": "Modelling salamander larvae abundance",
    "text": "Modelling salamander larvae abundance\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(brms)\n\nFor this example, we are going to model the relative abundance of blue-spotted salamander larvae in wetlands. We are interested in the effects of canopy cover, day of year, year, and study site. We we use a Poisson distribution with a log-link to model counts of larvae in wetlands and use an offset term to account for differences in the number of samples per wetland and the size of the wetland relative to maximum inundation. We will start with a frequentist approach using the glm() function in R.\n\nvp_amphibs &lt;- readRDS(here(\"data/vp_amphibs.rds\")) %&gt;%\n  mutate(year = factor(year)) %&gt;%\n  rename(site = sites)\n\nvp_AL &lt;- vp_amphibs %&gt;%\n  filter(species == \"BS\") %&gt;%\n  rowid_to_column()\n\nmod_freq &lt;- glm(count~\n                    scale(jday)+\n                    scale(canopy)+\n                    year+\n                    site+\n                    offset(log(samples/proparea)),\n                  data=vp_AL,\n                  family=\"poisson\")\nsummary(mod_freq)\n\n\nCall:\nglm(formula = count ~ scale(jday) + scale(canopy) + year + site + \n    offset(log(samples/proparea)), family = \"poisson\", data = vp_AL)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -1.01311    0.09254 -10.948  &lt; 2e-16 ***\nscale(jday)    -0.46280    0.03288 -14.075  &lt; 2e-16 ***\nscale(canopy)  -0.08285    0.03441  -2.408   0.0160 *  \nyear2020       -0.11099    0.07049  -1.575   0.1154    \nsiteGB2         0.81252    0.09970   8.150 3.64e-16 ***\nsiteGB7         0.30632    0.11010   2.782   0.0054 ** \nsiteGB8       -16.48719  359.62353  -0.046   0.9634    \nsiteGB9        -0.27215    0.12370  -2.200   0.0278 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2208.4  on 108  degrees of freedom\nResidual deviance: 1503.8  on 101  degrees of freedom\nAIC: 1857.9\n\nNumber of Fisher Scoring iterations: 13\n\n\nMost of these parameter estimates seem reasonable, but the effect size and standard error for the site GB8 variable are quite large. If we look at the total counts of larvae for each site we can see why.\n\nvp_AL %&gt;%\n  group_by(site) %&gt;%\n  summarise(total_count = sum(count)) %&gt;%\n  ggplot(aes(x = site, y = total_count)) +\n  geom_bar(stat = \"identity\") +\n  theme_bw()\n\n\n\n\nThis is what is known as complete separation, when the outcome variable is seemingly perfectly predicted by the predictor. Site GB8 has only two wetlands and blue-spotted salamander larvae were not found in either. Complete separation isn’t always an issue, but it can lead to errors in other parameter estimates and make model comparison difficult.\nUsing a Bayesian approach, we can make use of weak priors to keep our make sure our parameter estimates are realistic.\n\nprior &lt;- brms::set_prior(\"normal(0,1)\", class = \"b\")\n\n# Plot normal distribution\ntibble(x = seq(-4, 4, length.out = 1000)) %&gt;%\n  mutate(y = dnorm(x,0,1)) %&gt;%\n  ggplot(aes(x = x, y = y)) +\n  geom_line() +  # Plot the normal distribution curve\n  labs(title = \"Normal Distribution\",\n       x = \"x\",\n       y = \"Density\") +\n  theme_bw()\n\n\n\n\nWhat does this prior mean? The GB8 variable takes a value of 1 if the site is GB8 and 0 if it is not. Based on the prior distribution for GB8 above, we think the effect has a ~95% chance of being between -2 and 2. Since the expected count of salamander larvae is modelled with a log-link, this range of effects corresponds to a change in expected count between a factor of exp(-2) = 0.1 and a factor of exp(2) = 7.4, which seem reasonable for this system.\nIf we have a lot of data for GB8, the prior won’t have a huge impact on our posterior distribution. If we don’t have a lot of data, then the prior will make sure we don’t explore unreasonable values for the posterior.\nWe will now use the brm function to specify our model. Note that the syntax is similar to glm(), but we now include some extra information.\n\nmod_bayes &lt;- brms::brm(count~\n                    scale(jday)+\n                    scale(canopy)+\n                    year+\n                    site+\n                    offset(log(samples/proparea)),\n                  data=vp_AL,\n                  family=\"poisson\",\n                  prior = prior, ### our normal(0,1) prior\n                  iter = 2000, ### number of samples to draw from the posterior\n                  warmup = 1000, ### number of samples to discard from the start\n                  chains = 2, ### number of independent samples to draw\n                  cores = 2) ### number of processing cores to use on computer\n\nCompiling Stan program...\n\n\nStart sampling\n\n\nNow we can compare parameter estimates between the frequentist and Bayesian approaches\n\n# Extract parameter estimates\nglm_coefs &lt;- broom::tidy(mod_freq) %&gt;%\n  mutate(type = \"freq\") %&gt;%\n  select(term, estimate, std.error, type) \nbrms_coefs &lt;- as.data.frame(fixef(mod_bayes)) %&gt;%\n  rename(estimate = Estimate,\n         std.error = Est.Error) %&gt;%\n  mutate(term = glm_coefs$term,\n         type = \"bayes\") %&gt;%\n  select(term, estimate, std.error, type)\n  \n\nrbind(glm_coefs, brms_coefs) %&gt;%\n  mutate(estimate = round(estimate,2),\n         std.error = round(std.error,2)) %&gt;%\n  pivot_wider(names_from = \"type\", values_from = c(\"estimate\",\"std.error\")) %&gt;%\n    select(term, estimate_freq, std.error_freq, \n           estimate_bayes, std.error_bayes) %&gt;%\n  kableExtra::kbl(caption = \"Comparison of Model Parameters\",\n      col.names = c(\"Term\",\"Estimate\", \"Std. Error\",\"Estimate\", \"Std. Error\")) %&gt;%\n  kableExtra::add_header_above(c(\"\",\"Frequentist\"=2,\"Bayesian\"=2))\n\n\nComparison of Model Parameters\n\n\n\n\n\n\n\n\n\n\n\nFrequentist\n\n\nBayesian\n\n\n\nTerm\nEstimate\nStd. Error\nEstimate\nStd. Error\n\n\n\n\n(Intercept)\n-1.01\n0.09\n-1.03\n0.09\n\n\nscale(jday)\n-0.46\n0.03\n-0.46\n0.03\n\n\nscale(canopy)\n-0.08\n0.03\n-0.09\n0.04\n\n\nyear2020\n-0.11\n0.07\n-0.11\n0.07\n\n\nsiteGB2\n0.81\n0.10\n0.82\n0.10\n\n\nsiteGB7\n0.31\n0.11\n0.32\n0.11\n\n\nsiteGB8\n-16.49\n359.62\n-2.80\n0.55\n\n\nsiteGB9\n-0.27\n0.12\n-0.26\n0.12\n\n\n\n\n\n\n\nThe parameter estimates and standard deviations are similar between the two approaches, but the parameter estimate and standard deviation for GB8 are smaller in magnitude.\nLastly we’ll explore some diagnostics:\n\nsummary(mod_bayes)\n\n Family: poisson \n  Links: mu = log \nFormula: count ~ scale(jday) + scale(canopy) + year + site + offset(log(samples/proparea)) \n   Data: vp_AL (Number of observations: 109) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      -1.03      0.09    -1.21    -0.85 1.00     1248     1470\nscalejday      -0.46      0.03    -0.53    -0.40 1.00     1626     1523\nscalecanopy    -0.09      0.03    -0.16    -0.02 1.01     1739     1575\nyear2020       -0.11      0.07    -0.24     0.03 1.00     1694     1336\nsiteGB2         0.82      0.10     0.63     1.01 1.00     1128     1306\nsiteGB7         0.31      0.11     0.10     0.52 1.00     1172     1482\nsiteGB8        -2.81      0.53    -3.99    -1.84 1.00     1281     1171\nsiteGB9        -0.27      0.12    -0.50    -0.02 1.00     1241     1365\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nplot(mod_bayes, variable = c(\"b_scalecanopy\", \"b_siteGB8\"))"
  }
]